# Momentum Decoding For Neural Text Generation
**Authors**: Tian Lan and Yixuan Su

**[Contact]** If you have any questions, feel free to contact me via (lantiangmftby at gmail.com).

This repository contains code other related resources of our paper ["Momentum Decoding: Open-ended Text Generation As Graph Exploration"](https://arxiv.org/abs/).

****
If you find our paper and resources useful, please kindly leave a star and cite our papers. Thanks!

```bibtex
@article{su2022contrastiveiswhatyouneed,
  title={Contrastive Search Is What You Need For Neural Text Generation},
  author={Yixuan Su and Nigel Collier},
  journal={arXiv preprint arXiv:2210.14140},
  year={2022}
}
```

****

<span id='all_catelogue'/>

### Catalogue:
* <a href='#introduction'>1. Introduction</a>
* <a href='#reproduce_examples'>2. Reproducing Examples Provided in the Paper</a>
    * <a href='#use_transformers'>2.1. Using Huggingface Transformers</a>
        * <a href='#install_transformers'>2.1.1. Install Transformers Package</a>
        * <a href='#transformers_table_1'>2.1.2. Example in Table 1</a>
        * <a href='#transformers_table_8'>2.1.3. Example in Table 8 at Appendix A</a>
        * <a href='#transformers_table_9'>2.1.4. Example in Table 9 at Appendix A</a>
    * <a href='#use_simctg'>2.2. Using SimCTG Package</a>
        * <a href='#install_simctg'>2.2.1. Install SimCTG Package</a>
        * <a href='#simctg_table_1'>2.2.2. Example in Table 1</a>
        * <a href='#simctg_table_8'>2.2.3. Example in Table 8 at Appendix A</a>
        * <a href='#simctg_table_9'>2.2.4. Example in Table 9 at Appendix A</a>
* <a href='#LMs_isotropy'>3. Measuring Isotropy of LMs</a>
* <a href='#open_ended_text_generation'>4. Open-ended Text Generation</a>
* <a href='#code_generation'>5. Code Generation</a>
* <a href='#machine_translation'>6. Machine Translation</a>
    
****

<span id='introduction'/>

#### 1. Introduction: <a href='#all_catelogue'>[Back to Top]</a>

Open-ended text generation with autoregressive language models (LMs) is an indispensable component in various NLP applications. Typical examples include dialogue systems , contextual text completion, story generation, etc.

Conventional maximization-based methods for this task, such as greedy search and beam search, often lead to the degeneration problem, i.e. the generated text is unnatural and contains undesirable repetitions.
Existing solutions for this problem can be divided into two categories: 
(1) Stochastic methods, e.g. top-$k$ and nucleus sampling, introduce randomness to avoid undesirable repetitions. However, the intrinsic stochasticity of these sampling approaches often leads to semantic incoherence and topic drift in the generated text.
(2) Deteriminstic method, i.e. contrastive search, relies on a one-step look-ahead mechanism to encourage diverse generations. While obtaining superior performances, such look-ahead operation demands extra computational overhead.

In this study, we perceive open-ended text generation from a new perspective. Specifically, we view it as an exploration process within a directed graph.
Therefore, it allows us to formulate the phenomenon of degeneration as circular loops within the directed graph. In this ![Figure](img/overview.png), we provide an illustration in which the LM generates text given a prefix of three tokens, i.e. [1,2,3], and gets stuck in the circular loops, i.e. repetitions, of [2,3,7,8]. Intuitively, such degeneration can be addressed if the tendency of the LM to stay in the circular loop can be _properly_ discouraged, therefore allowing the LM to jump out of the loop at the correct position and produce text with _natural_ repetitions. Based on this motivation, we propose a novel decoding method---_momentum decoding_---which encourages the LM to greedily explore new nodes outside the current graph. Meanwhile, it also allows the LM to return to the existing nodes but with a momentum downgraded by a pre-defined resistance function. 

****


<span id='reproduce_examples'/>


#### 2. Reproducing Examples Provided in the Paper: <a href='#all_catelogue'>[Back to Top]</a>
In this section, we demonstrate two ways of reproducing the examples generated by contrastive search provided in our paper.

<span id='use_transformers'/>

##### 2.1. Using Huggingface Transformers: <a href='#all_catelogue'>[Back to Top]</a>

<span id='install_transformers'/>

###### 2.1.1. Install Transformers Package: 

To install `transformers` from the source, please run the following command:
```yaml
pip install "transformers>=4.24.0"
```

<span id='transformers_table_1'/>

###### 2.1.2. Example in Table 1: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 1, please run the following command:
```python
# load the LMs
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
model_name = 'gpt2-large'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)
model.eval()

# prepare the prefix
prefix_text = r"Kobe Bryant is"
inputs = tokenizer(prefix_text, return_tensors='pt')

# generate the result with contrastive search
output = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_length=256)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output[0], skip_special_tokens=True))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------
Kobe Bryant is the best player in the world.

I know this is a bold statement to make, but it's true. He may have won a lot of games, but his
impact on the game is immeasurable. Kobe is one of the most popular players in the world, and
that's no small feat when you consider how many people play video games, watch sports, listen
to music, and have social media accounts.

Kobe has made a career out of being a role model for young people, and he does an amazing job
of that every time he steps on the court. His career stats are impressive, but what makes him
stand out is the way he goes about his business. When he's on the court, he's the center of
attention, and it's easy to see why.

The Lakers' All-Star point guard wears a smile on his face like no other. He's always in the
right place at the right time, making the right play at the right time, and doing the right
thing in the right way. He's not afraid to get his hands dirty, and he never shies away from
a physical confrontation.

Kobe's personality is something that can't be taught, and it's a big reason why...
----------------------------------------------------------------------------------------------------
```
</details>


<span id='transformers_table_8'/>

###### 2.1.3. Example in Table 8 at Appendix A: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 8 at Appendix A, please run the following command:
```python
# load the LMs
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
model_name = 'gpt2-large'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)
model.eval()

# prepare the prefix
prefix_text = r'DeepMind Company is'
inputs = tokenizer(prefix_text, return_tensors='pt')

# generate the result with contrastive search
output = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_length=512)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output[0], skip_special_tokens=True))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------  
DeepMind Company is a leader in artificial intelligence (AI). We have a long history of working
with companies such as Google, Facebook, Amazon, and Microsoft to build products that improve
people's lives, and today we are excited to announce that DeepMind's AlphaGo program has won the
game of Go, becoming the first program to defeat a professional Go player.

The victory is a testament to the power of deep learning, and to the incredible work of our
research team, which has been at the forefront of AI research for the past five years. AlphaGo
is one of the most advanced Go programs ever created, and its performance is an important step
towards the goal of human-level AI.

"This is the culmination of a decade of hard work," said Andy Ng, co-founder and CTO of DeepMind.
"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that
can be used in a wide range of applications and to help people live better lives."

DeepMind's work on Go began in 2010, when it began to train a neural network to play Go using
millions of games played by top Go players around the world. Since then, the team has refined the
algorithm, adding more and more layers of reinforcement learning to make it better at recognizing
patterns and making decisions based on those patterns. In the past year and a half, the team has
made significant progress in the game, winning a record-tying 13 games in a row to move into the
top four of the world rankings.

"The game of Go is a complex game in which players have to be very careful not to overextend their
territory, and this is something that we have been able to improve over and over again," said
Dr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. "We are very proud of our
team's work, and we hope that it will inspire others to take the next step in their research and
apply the same techniques to other problems."

In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a
number of different games, including poker, Go, and chess. This AI system, called Tarsier, was
developed in partnership with Carnegie Mellon University and the University of California, 
Berkeley, and is being used to teach computer vision and machine learning to identify objects in
images and recognize speech in natural language. Tarsier has been trained to play the game of Go
and other games on a number of different platforms...
----------------------------------------------------------------------------------------------------
```
</details>


<span id='transformers_table_9'/>

###### 2.1.4. Example in Table 9 at Appendix A: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 9 at Appendix A, please run the following command:
```python
# load the LMs
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
model_name = 'gpt2-large'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)
model.eval()

# prepare the prefix
prefix_text = r"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
inputs = tokenizer(prefix_text, return_tensors='pt')

# generate the result with contrastive search
output = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_length=512)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output[0], skip_special_tokens=True))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------  
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously
unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact
that the unicorns spoke perfect English.

According to the BBC, a team of scientists led by Dr David MacKay, from the University of
Bristol, spent two years searching for the unicorn herd, which they discovered during a survey
of the area.

"It's a very rare find," MacKay told the BBC. "There are a few in the Himalayas, but this is
the first time we've been able to find one in such a remote area."

The team was surprised to find a herd of unicorns living in a region that has been known to be
a hotbed of poaching, with many of the animals poached for their horns, which are used in
traditional Chinese medicine to treat everything from rheumatism to cancer.

"We knew that the area was rich in rhino horn, but we had no idea how many there were, or what
they were doing there," MacKay said. "This is an area of high poaching pressure, and we wanted
to find out what was going on."

In order to do so, the team used GPS collars to track the animals as they moved around the
mountain and the surrounding area. The GPS data was then compared with information gathered
from local villagers, who had a wealth of information about the animals' movements, including
where they were eating, what they were doing at night, and how much time they spent in the
mountains each day.

After analyzing the data, the team determined that the herd consisted of at least three species
of unicorns, including a male and two females. One of the females was the mother of the male,
and the other two were her daughters. All three had the same horn color, which is believed to
be a sign of purity in the animal kingdom.

While the discovery is exciting, it's not the first time scientists have discovered an animal
that speaks English. Last year, scientists discovered a species of porcupine that can be heard
by humans, and has been dubbed "Porcupine Man" for his ability to converse with the human race.
----------------------------------------------------------------------------------------------------
```
</details>


<span id='use_simctg'/>

##### 2.2. Using SimCTG Package: <a href='#all_catelogue'>[Back to Top]</a>

<span id='install_simctg'/>

###### 2.2.1. Install SimCTG Package: 

To install the package, please run the following command:

```yaml
pip install simctg --upgrade
```

<span id='simctg_table_1'/>

###### 2.2.2. Example in Table 1: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 1, please run the following command:
```python
# load the LMs
import torch
from simctg.simctggpt import SimCTGGPT
model_name = r'gpt2-large'
model = SimCTGGPT(model_name)
model.eval()
tokenizer = model.tokenizer
eos_token_id = tokenizer.eos_token_id

# prepare the prefix
prefix_text = r"Kobe Bryant is"
tokens = tokenizer.tokenize(prefix_text)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.LongTensor(input_ids).view(1,-1)

# generate the result with contrastive search
beam_width, alpha, decoding_len = 4, 0.6, 256
output = model.fast_contrastive_search(input_ids=input_ids, beam_width=beam_width, 
                                       alpha=alpha, decoding_len=decoding_len,
                                      end_of_sequence_token_id = eos_token_id, early_stop = True) 
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------
Kobe Bryant is the best player in the world.

I know this is a bold statement to make, but it's true. He may have won a lot of games, but his
impact on the game is immeasurable. Kobe is one of the most popular players in the world, and
that's no small feat when you consider how many people play video games, watch sports, listen
to music, and have social media accounts.

Kobe has made a career out of being a role model for young people, and he does an amazing job
of that every time he steps on the court. His career stats are impressive, but what makes him
stand out is the way he goes about his business. When he's on the court, he's the center of
attention, and it's easy to see why.

The Lakers' All-Star point guard wears a smile on his face like no other. He's always in the
right place at the right time, making the right play at the right time, and doing the right
thing in the right way. He's not afraid to get his hands dirty, and he never shies away from
a physical confrontation.

Kobe's personality is something that can't be taught, and it's a big reason why...
----------------------------------------------------------------------------------------------------
```
</details>




<span id='simctg_table_8'/>

###### 2.2.3. Example in Table 8 at Appendix A: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 8 at Appendix A, please run the following command:
```python
# load the LMs
import torch
from simctg.simctggpt import SimCTGGPT
model_name = r'gpt2-large'
model = SimCTGGPT(model_name)
model.eval()
tokenizer = model.tokenizer
eos_token_id = tokenizer.eos_token_id

# prepare the prefix
prefix_text = r"DeepMind Company is"
tokens = tokenizer.tokenize(prefix_text)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.LongTensor(input_ids).view(1,-1)

# generate the result with contrastive search
beam_width, alpha, decoding_len = 4, 0.6, 512
output = model.fast_contrastive_search(input_ids=input_ids, beam_width=beam_width, 
                                       alpha=alpha, decoding_len=decoding_len,
                                      end_of_sequence_token_id = eos_token_id, early_stop = True) 
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------  
DeepMind Company is a leader in artificial intelligence (AI). We have a long history of working
with companies such as Google, Facebook, Amazon, and Microsoft to build products that improve
people's lives, and today we are excited to announce that DeepMind's AlphaGo program has won the
game of Go, becoming the first program to defeat a professional Go player.

The victory is a testament to the power of deep learning, and to the incredible work of our
research team, which has been at the forefront of AI research for the past five years. AlphaGo
is one of the most advanced Go programs ever created, and its performance is an important step
towards the goal of human-level AI.

"This is the culmination of a decade of hard work," said Andy Ng, co-founder and CTO of DeepMind.
"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that
can be used in a wide range of applications and to help people live better lives."

DeepMind's work on Go began in 2010, when it began to train a neural network to play Go using
millions of games played by top Go players around the world. Since then, the team has refined the
algorithm, adding more and more layers of reinforcement learning to make it better at recognizing
patterns and making decisions based on those patterns. In the past year and a half, the team has
made significant progress in the game, winning a record-tying 13 games in a row to move into the
top four of the world rankings.

"The game of Go is a complex game in which players have to be very careful not to overextend their
territory, and this is something that we have been able to improve over and over again," said
Dr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. "We are very proud of our
team's work, and we hope that it will inspire others to take the next step in their research and
apply the same techniques to other problems."

In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a
number of different games, including poker, Go, and chess. This AI system, called Tarsier, was
developed in partnership with Carnegie Mellon University and the University of California, 
Berkeley, and is being used to teach computer vision and machine learning to identify objects in
images and recognize speech in natural language. Tarsier has been trained to play the game of Go
and other games on a number of different platforms...
----------------------------------------------------------------------------------------------------
```
</details>

<span id='simctg_table_9'/>

###### 2.2.4. Example in Table 9 at Appendix A: <a href='#all_catelogue'>[Back to Top]</a>

To reproduce our example provided in Table 9 at Appendix A, please run the following command:
```python
# load the LMs
import torch
from simctg.simctggpt import SimCTGGPT
model_name = r'gpt2-large'
model = SimCTGGPT(model_name)
model.eval()
tokenizer = model.tokenizer
eos_token_id = tokenizer.eos_token_id

# prepare the prefix
prefix_text = r"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
tokens = tokenizer.tokenize(prefix_text)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.LongTensor(input_ids).view(1,-1)

# generate the result with contrastive search
beam_width, alpha, decoding_len = 4, 0.6, 512
output = model.fast_contrastive_search(input_ids=input_ids, beam_width=beam_width, 
                                       alpha=alpha, decoding_len=decoding_len,
                                      end_of_sequence_token_id = eos_token_id, early_stop = True) 
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
print("" + 100 * '-')
```

<details>
<summary><b>Model Output: [click to expand]</b></summary>
  
```
Output:
----------------------------------------------------------------------------------------------------  
In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously
unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact
that the unicorns spoke perfect English.

According to the BBC, a team of scientists led by Dr David MacKay, from the University of
Bristol, spent two years searching for the unicorn herd, which they discovered during a survey
of the area.

"It's a very rare find," MacKay told the BBC. "There are a few in the Himalayas, but this is
the first time we've been able to find one in such a remote area."

The team was surprised to find a herd of unicorns living in a region that has been known to be
a hotbed of poaching, with many of the animals poached for their horns, which are used in
traditional Chinese medicine to treat everything from rheumatism to cancer.

"We knew that the area was rich in rhino horn, but we had no idea how many there were, or what
they were doing there," MacKay said. "This is an area of high poaching pressure, and we wanted
to find out what was going on."

In order to do so, the team used GPS collars to track the animals as they moved around the
mountain and the surrounding area. The GPS data was then compared with information gathered
from local villagers, who had a wealth of information about the animals' movements, including
where they were eating, what they were doing at night, and how much time they spent in the
mountains each day.

After analyzing the data, the team determined that the herd consisted of at least three species
of unicorns, including a male and two females. One of the females was the mother of the male,
and the other two were her daughters. All three had the same horn color, which is believed to
be a sign of purity in the animal kingdom.

While the discovery is exciting, it's not the first time scientists have discovered an animal
that speaks English. Last year, scientists discovered a species of porcupine that can be heard
by humans, and has been dubbed "Porcupine Man" for his ability to converse with the human race.
----------------------------------------------------------------------------------------------------
```
</details>



****

<span id='LMs_isotropy'/>

##### 3. Measuring Isotropy of LMs: <a href='#all_catelogue'>[Back to Top]</a>

The detailed tutorial on measuring the isotropy of LMs is provided [[here]](./isotropy_analysis/).

**[Note]** To replicate our experimental results, please make sure you have installed the environment as
```yaml
pip install simctg --upgrade
```


****

<span id='open_ended_text_generation'/>

##### 4. Open-ended Text Generation: <a href='#all_catelogue'>[Back to Top]</a>

The detailed tutorial on the experiments of open-ended text generation is provided [[here]](./open_ended_generation/).

**[Note]** To replicate our experimental results, please make sure you have installed the environment as
```yaml
pip install simctg --upgrade
```

****

<span id='code_generation'/>

##### 5. Code Generation: <a href='#all_catelogue'>[Back to Top]</a>

The detailed tutorial on the experiments of code generation is provided [[here]](./code_generation/).

**[Note]** To replicate our experimental results, please make sure you have installed the environment as
```yaml
pip install simctg --upgrade
```

****

<span id='machine_translation'/>

##### 6. Machine Translation: <a href='#all_catelogue'>[Back to Top]</a>

The detailed tutorial on the experiments of machine translation is provided [[here]](./translation/).

**[Note]** To replicate our experimental results, please make sure you have installed the environment as
```yaml
pip install simctg --upgrade
```


